import os
import asyncio
from typing import List, Dict, Any
from dotenv import load_dotenv
from ragas.metrics import (
    ResponseRelevancy,
    Faithfulness,
    LLMContextPrecisionWithoutReference
)
from ragas import SingleTurnSample
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

load_dotenv()

class RAGASEvaluator:
    def __init__(self):
        self.llm = LangchainLLMWrapper(ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1
        ))
        self.embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())
        
        self.response_relevancy = ResponseRelevancy(
            llm=self.llm,
            embeddings=self.embeddings
        )
        self.faithfulness = Faithfulness(llm=self.llm)
        self.context_precision = LLMContextPrecisionWithoutReference(llm=self.llm)
    
    async def evaluate_response(
        self, 
        user_input: str, 
        response: str, 
        retrieved_contexts: List[str]
    ) -> Dict[str, float]:
        sample = SingleTurnSample(
            user_input=user_input,
            response=response,
            retrieved_contexts=retrieved_contexts
        )
        
        try:
            response_relevancy_score = await self.response_relevancy.single_turn_ascore(sample)
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ Response Relevancy: {e}")
            response_relevancy_score = 0.0
        
        try:
            faithfulness_score = await self.faithfulness.single_turn_ascore(sample)
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ Faithfulness: {e}")
            faithfulness_score = 0.0
        
        try:
            context_precision_score = await self.context_precision.single_turn_ascore(sample)
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ Context Precision: {e}")
            context_precision_score = 0.0
        
        return {
            "response_relevancy": response_relevancy_score,
            "faithfulness": faithfulness_score,
            "context_precision": context_precision_score
        }
    
    def print_metrics(self, metrics: Dict[str, float], user_input: str):
        print("\n" + "="*60)
        print("üìä RAGAS –ú–ï–¢–†–ò–ö–ò –û–¶–Ü–ù–ö–ò –í–Ü–î–ü–û–í–Ü–î–Ü")
        print("="*60)
        print(f"–ó–∞–ø–∏—Ç: {user_input}")
        print("-"*60)
        
        print(f"üéØ Response Relevancy: {metrics['response_relevancy']:.3f}")
        print("   (–ù–∞—Å–∫—ñ–ª—å–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –¥–æ –∑–∞–ø–∏—Ç—É)")
        
        print(f"‚úÖ Faithfulness: {metrics['faithfulness']:.3f}")
        print("   (–ù–∞—Å–∫—ñ–ª—å–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—î—Ç—å—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)")
        
        print(f"üîç Context Precision: {metrics['context_precision']:.3f}")
        print("   (–Ø–∫—ñ—Å—Ç—å –æ—Ç—Ä–∏–º–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)")
        
        avg_score = sum(metrics.values()) / len(metrics)
        print(f"\nüìà –°–µ—Ä–µ–¥–Ω—ñ–π –±–∞–ª: {avg_score:.3f}")
        
        if avg_score >= 0.8:
            print("üü¢ –í—ñ–¥–º—ñ–Ω–Ω–∞ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ!")
        elif avg_score >= 0.6:
            print("üü° –•–æ—Ä–æ—à–∞ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ")
        elif avg_score >= 0.4:
            print("üü† –ó–∞–¥–æ–≤—ñ–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ")
        else:
            print("üî¥ –ü–æ—Ç—Ä–µ–±—É—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
        
        print("="*60 + "\n")

def evaluate_rag_response(user_input: str, response: str, retrieved_contexts: List[str]):
    evaluator = RAGASEvaluator()
    
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()
        
        metrics = loop.run_until_complete(
            evaluator.evaluate_response(user_input, response, retrieved_contexts)
        )
        evaluator.print_metrics(metrics, user_input)
        return metrics
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ü—ñ–Ω—Ü—ñ RAGAS –º–µ—Ç—Ä–∏–∫: {e}")
        return None

if __name__ == "__main__":
    test_input = "–°–∫—ñ–ª—å–∫–∏ –∫–æ—à—Ç—É—î –º–∞–Ω—ñ–∫—é—Ä?"
    test_response = "–ú–∞–Ω—ñ–∫—é—Ä –∫–æ—à—Ç—É—î 350 –≥—Ä–Ω."
    test_contexts = ["–ü—Ä–∞–π—Å-–ª–∏—Å—Ç: –ú–∞–Ω—ñ–∫—é—Ä - 350 –≥—Ä–Ω, –ü–µ–¥–∏–∫—é—Ä - 400 –≥—Ä–Ω"]
    
    evaluate_rag_response(test_input, test_response, test_contexts) 